# sabim_ai_tools/models.py
import ollama

class Models:
    def __init__(self, model, prompt, api_key=None, context = None):
        self.model = model
        self.prompt = prompt
        self.api_key = api_key
        self.context = context
        

    def model_call(self):
        """
        Initializes and returns an instance of the LLM model.

        Returns:
            object: The Llama3 model instance.
        """
        if self.model.startswith("llama3"):
            return ollama
        else:
            raise ValueError("Unsupported model. Only Llama3 and Google GenAI are supported.")

    def chat_interaction(self):
        """ 
        Interacts with an LLM model to generate a response based on a provided prompt and context.

        Args:
            title (str): The title of the article.
            abstract (str): The abstract of the article.

        Returns:
            str: The content generated by the AI model.
        """
        model = self.model_call()
        input_ai = ()
        response = model.chat(model=self.model, messages=[{"role": "user", "content": input_ai}])
        return response["message"]["content"]
    
    